---
phase: 02-analytics-engine
plan: 03
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - components/session_quality.py
  - components/__init__.py
  - tests/unit/test_analytics_utils.py
autonomous: true

must_haves:
  truths:
    - "User sees session quality score (0-100) summarizing consistency and improvement"
    - "Session quality score is interpretable with component breakdown"
    - "All 5 new analytics components are importable from components package"
    - "Analytics utilities have unit test coverage"
  artifacts:
    - path: "components/session_quality.py"
      provides: "Session quality scoring component"
      exports: ["render_session_quality"]
    - path: "components/__init__.py"
      provides: "Updated component exports including all 5 new analytics components"
      contains: "render_dispersion_chart"
    - path: "tests/unit/test_analytics_utils.py"
      provides: "Unit tests for analytics utility functions"
      min_lines: 60
  key_links:
    - from: "components/session_quality.py"
      to: "analytics/utils.py"
      via: "import normalize_score, normalize_inverse"
      pattern: "from analytics.utils import"
    - from: "components/__init__.py"
      to: "components/dispersion_chart.py"
      via: "import render_dispersion_chart"
      pattern: "from .dispersion_chart import render_dispersion_chart"
---

<objective>
Implement session quality scoring (ANLYT-05), update components package exports, and add unit tests for analytics utilities.

Purpose: Complete the analytics engine with an interpretable session quality score, make all 5 new components accessible via the components package, and ensure shared analytics utilities have test coverage.

Output: `components/session_quality.py`, updated `components/__init__.py`, `tests/unit/test_analytics_utils.py`
</objective>

<execution_context>
@/Users/max1/.claude/get-shit-done/workflows/execute-plan.md
@/Users/max1/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-analytics-engine/02-RESEARCH.md
@.planning/phases/02-analytics-engine/02-01-SUMMARY.md

Key codebase references:
@components/__init__.py (current exports — needs update)
@components/heatmap_chart.py (established render_* pattern)
@analytics/utils.py (shared utilities from plan 01: normalize_score, normalize_inverse)
@golf_db.py (session_stats table columns: avg_carry, avg_smash, std_face_angle, std_club_path, avg_strike_distance, std_strike_distance, best_carry, shot_count)
@golf_db.py (get_session_metrics function at line ~2173)
@tests/conftest.py (existing test fixtures)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create session quality component with composite scoring</name>
  <files>components/session_quality.py</files>
  <action>
Implement `render_session_quality(session_stats: dict, historical_stats: list[dict] = None) -> None`:

Note: This component takes a session_stats dict (from golf_db.get_session_metrics) rather than a raw DataFrame, since session_stats already has pre-computed aggregates.

**Internal scoring function:**
`_calculate_quality_score(stats: dict, historical: list[dict] = None) -> dict`

Components (weighted):
1. **Consistency (40%):** Lower standard deviations = better
   - Sub-metrics: std_face_angle (weight 0.5), std_club_path (weight 0.3), std_strike_distance (weight 0.2)
   - Normalize each inversely (lower = higher score):
     - std_face_angle: min=0.5, max=5.0 (recreational player typical range)
     - std_club_path: min=0.5, max=4.0
     - std_strike_distance: min=0.1, max=0.5 (inches)
   - If any sub-metric is None/missing, redistribute weight to remaining sub-metrics
   - Use analytics.utils.normalize_inverse for each

2. **Performance (30%):** Higher smash, speed, carry = better
   - Sub-metrics: avg_smash (weight 0.5), avg_ball_speed (weight 0.3), avg_carry (weight 0.2)
   - Normalize each:
     - avg_smash: min=1.30, max=1.50
     - avg_ball_speed: min=100, max=170 (mph)
     - avg_carry: min=100, max=280 (yds) — this is approximate; personalized in v2
   - If any sub-metric is None/missing, redistribute weight
   - Use analytics.utils.normalize_score for each

3. **Improvement (30%):** Comparison to historical average (if available)
   - If historical_stats is None or empty: default to 50 (neutral)
   - Otherwise: compare current session's avg_carry to historical mean avg_carry
   - improvement_pct = ((current - historical_mean) / historical_mean) * 100
   - Normalize: -10% = 0, 0% = 50, +10% = 100 (linear scale, clamped)

Return dict: overall_score (0-100 rounded to 1 decimal), consistency_score, performance_score, improvement_score, component_details (dict with each sub-metric's contribution).

**Main render function:**
1. Subheader "Session Quality Score"
2. If session_stats is None/empty: st.info("No session metrics available")
3. If shot_count < 5: st.warning("Need 5+ shots for meaningful quality score")
4. Calculate quality score
5. Display overall score prominently:
   - Use st.metric with overall_score, delta from 50 (neutral baseline)
   - Color interpretation: 0-30 "Needs work", 31-50 "Below average", 51-70 "Solid session", 71-85 "Great session", 86-100 "Exceptional"
6. Show component breakdown in st.columns(3):
   - Consistency: score/100 with progress bar (st.progress)
   - Performance: score/100 with progress bar
   - Improvement: score/100 with progress bar (or "No baseline" if first session)
7. Show st.expander "Score Breakdown" with detailed sub-metric contributions:
   - Table showing each metric, its value, its normalized score, and its weight
   - Explain: "Consistency (40%): Based on standard deviation of face angle, club path, and strike location"
   - Explain: "Performance (30%): Based on smash factor, ball speed, and carry distance"
   - Explain: "Improvement (30%): Compared to your historical average"
8. Actionable tip based on lowest component:
   - If consistency lowest: "Focus on repeating your setup and swing tempo for more consistent results."
   - If performance lowest: "Work on strike quality (center contact) to improve ball speed and carry."
   - If improvement lowest: "Your consistency is good but you're not improving. Consider working with a coach or trying new drills."
  </action>
  <verify>
`python -m py_compile components/session_quality.py` succeeds.
`python -c "from components.session_quality import render_session_quality; print('OK')"` prints OK.
  </verify>
  <done>Session quality score (0-100) displayed with component breakdown, progress bars, sub-metric details, and actionable coaching tips. Handles missing data and first-session scenarios gracefully.</done>
</task>

<task type="auto">
  <name>Task 2: Update components __init__.py and add analytics utility tests</name>
  <files>components/__init__.py, tests/unit/test_analytics_utils.py</files>
  <action>
**components/__init__.py:**
Add imports and __all__ entries for all 5 new analytics components:
- `from .dispersion_chart import render_dispersion_chart`
- `from .distance_table import render_distance_table`
- `from .miss_tendency import render_miss_tendency`
- `from .progress_tracker import render_progress_tracker`
- `from .session_quality import render_session_quality`

Add all 5 to __all__ list. Keep existing imports unchanged.

**tests/unit/test_analytics_utils.py:**
Create unit tests for analytics/utils.py functions using unittest.TestCase:

1. `TestFilterOutliersIQR`:
   - test_filters_outliers: DataFrame with known outliers; verify outliers removed
   - test_empty_dataframe: Empty df returns empty df
   - test_all_nan_column: Column of all NaN returns original df
   - test_small_sample: Fewer than 3 values returns unfiltered
   - test_custom_multiplier: multiplier=3.0 keeps more data than 1.5

2. `TestCheckMinSamples`:
   - test_sufficient_samples: 10 items, min_n=3 -> (True, "")
   - test_insufficient_samples: 2 items, min_n=3 -> (False, message with context)
   - test_custom_min_n: min_n=5 with 4 items -> insufficient

3. `TestNormalizeScore`:
   - test_normal_value: 50 in range [0, 100] -> 50.0
   - test_below_min: -10 in range [0, 100] -> 0.0 (clamped)
   - test_above_max: 150 in range [0, 100] -> 100.0 (clamped)
   - test_equal_min_max: min=max -> 50.0

4. `TestNormalizeInverse`:
   - test_low_value_scores_high: 0.5 in range [0.5, 5.0] -> 100.0
   - test_high_value_scores_low: 5.0 in range [0.5, 5.0] -> 0.0

5. `TestCalculateDistanceStats`:
   - test_basic_stats: DataFrame with 10 Driver shots; verify median, q25, q75, sample_size, confidence='high'
   - test_insufficient_data: DataFrame with 2 shots; returns None
   - test_confidence_levels: 3 shots='low', 7 shots='medium', 15 shots='high'

Use pd.DataFrame and np for test data. Import from analytics.utils. Follow existing test patterns in tests/unit/.
  </action>
  <verify>
`python -m py_compile components/__init__.py && python -m py_compile tests/unit/test_analytics_utils.py` succeeds.
`python -c "from components import render_dispersion_chart, render_distance_table, render_miss_tendency, render_progress_tracker, render_session_quality; print('All 5 OK')"` prints "All 5 OK".
`python -m unittest tests.unit.test_analytics_utils -v` — all tests pass.
`python -m unittest discover -s tests` — all existing tests still pass.
  </verify>
  <done>All 5 new analytics components exported from components package. Analytics utility functions have 15+ unit tests covering edge cases, normalization, IQR filtering, and distance stats. Zero regressions in existing test suite.</done>
</task>

</tasks>

<verification>
1. `python -m py_compile components/__init__.py components/session_quality.py tests/unit/test_analytics_utils.py` — all pass
2. `python -c "from components import render_dispersion_chart, render_distance_table, render_miss_tendency, render_progress_tracker, render_session_quality"` — all 5 importable
3. `python -m unittest tests.unit.test_analytics_utils -v` — all utility tests pass
4. `python -m unittest discover -s tests` — full test suite passes (zero regressions)
5. Session quality score handles missing data, first-session scenario, and shows component breakdown
</verification>

<success_criteria>
- Session quality score (0-100) with consistency/performance/improvement breakdown and actionable tips
- All 5 new components importable from components package
- Analytics utility functions have 15+ unit tests with edge case coverage
- Full test suite passes with zero regressions
</success_criteria>

<output>
After completion, create `.planning/phases/02-analytics-engine/02-03-SUMMARY.md`
</output>
