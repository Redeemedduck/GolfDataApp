---
phase: 01-foundation-stability
plan: 03
type: execute
wave: 2
depends_on: ["02"]
files_modified:
  - ml/train_models.py
  - golf_db.py
  - tests/unit/test_ml_models.py
  - tests/test_golf_db.py
autonomous: true

must_haves:
  truths:
    - "Loaded models include training metadata (date, sample size, accuracy)"
    - "Old models are backward compatible with new model loader"
    - "Session metrics table is populated automatically after each import"
  artifacts:
    - path: "ml/train_models.py"
      provides: "Model versioning with backward compatibility"
      contains: "ModelMetadata.*version"
      min_lines: 500
    - path: "golf_db.py"
      provides: "Session metrics population logic"
      exports: ["update_session_metrics", "get_session_metrics"]
      contains: "INSERT.*session_stats"
  key_links:
    - from: "ml/train_models.py"
      to: "models/*.metadata.json"
      via: "saves metadata alongside model file"
      pattern: "json\\.dump.*metadata"
    - from: "golf_db.py"
      to: "session_stats table"
      via: "computes and upserts aggregate stats"
      pattern: "INSERT OR REPLACE.*session_stats"
---

<objective>
Validate model versioning infrastructure and implement session metrics population logic.

Purpose: Ensure models are traceable with metadata. Provide efficient aggregate stats for trend analysis without recalculating from raw shots.

Output: Model versioning with backward compatibility validated, session metrics table populated automatically.
</objective>

<execution_context>
@/Users/max1/.claude/get-shit-done/workflows/execute-plan.md
@/Users/max1/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/research/PITFALLS.md

# Current implementation
@ml/train_models.py
@golf_db.py
</context>

<tasks>

<task type="auto">
  <name>Validate and enhance model versioning in train_models.py</name>
  <files>ml/train_models.py</files>
  <action>
Review and enhance existing ModelMetadata implementation for robustness.

Changes:
1. Verify `save_model()` saves metadata to `{model_path}.metadata.json` (already implemented — confirm it works)
2. Verify `load_model()` handles missing metadata gracefully (already implemented — confirm backward compatibility)
3. Add validation to `load_model()` to check metadata compatibility:
   - If metadata exists, verify `features` list matches model expectations
   - If feature count mismatch, log warning but don't crash
   - Set `DistancePredictor._feature_names` from metadata or defaults

4. Add `get_model_info(model_path: Path) -> Optional[ModelMetadata]` function:
   - Returns metadata without loading full model (fast check)
   - Returns None if metadata doesn't exist (backward compatibility)
   - Use case: UI showing model details before prediction

5. Update `DistancePredictor.load()` to validate metadata after loading:
   - Check if metadata.features exists and matches model
   - Log warning if metadata missing or mismatched
   - Fallback to DEFAULT_FEATURE_NAMES if needed

No breaking changes — must be backward compatible with models trained before metadata was added.
  </action>
  <verify>
```bash
# Test loading model without metadata (backward compat)
python -c "from ml.train_models import DistancePredictor; p = DistancePredictor(); p.load(); print('Loaded:', p.metadata)"

# Test get_model_info function
python -c "from ml.train_models import get_model_info; from pathlib import Path; info = get_model_info(Path('models/distance_model.joblib')); print(info)"
```
Both should work without crashes.
  </verify>
  <done>
- `load_model()` validates metadata compatibility
- `get_model_info()` function added for lightweight metadata access
- Backward compatibility maintained for models without metadata
- Feature name mismatches log warnings but don't crash
  </done>
</task>

<task type="auto">
  <name>Implement session metrics population in golf_db.py</name>
  <files>golf_db.py</files>
  <action>
Add functions to populate and query the existing `session_stats` table.

Create `update_session_metrics(session_id: str) -> None`:
1. Query all shots for given session_id
2. Compute aggregates:
   - shot_count: COUNT(*)
   - clubs_used: DISTINCT(club) as comma-separated string
   - avg_carry, avg_total, avg_ball_speed, avg_club_speed, avg_smash: AVG()
   - best_carry: MAX(carry)
   - avg_face_angle, std_face_angle: AVG(), STDDEV()
   - avg_club_path, std_club_path: AVG(), STDDEV()
   - avg_face_to_path: AVG(face_angle - club_path)
   - avg_strike_distance: AVG(SQRT(impact_x^2 + impact_y^2))
   - std_strike_distance: STDDEV(SQRT(impact_x^2 + impact_y^2))
3. Use `INSERT OR REPLACE INTO session_stats` with all computed values
4. Set updated_at to CURRENT_TIMESTAMP

Create `get_session_metrics(session_id: str) -> Optional[dict]`:
1. Query session_stats table for given session_id
2. Return dict with all metrics
3. Return None if session not found

Create `update_all_session_metrics() -> int`:
1. Get all unique session_ids from shots table
2. Call update_session_metrics() for each
3. Return count of sessions updated

Hook into existing write operations:
- Call `update_session_metrics(session_id)` after `add_shot()` completes
- Call `update_session_metrics(session_id)` after `delete_shot()` completes
- Call `update_session_metrics(session_id)` after session import completes

Use pandas for efficient computation (already imported in golf_db.py).
  </action>
  <verify>
```bash
# Test metrics computation
python -c "import golf_db; golf_db.init_db(); golf_db.update_all_session_metrics(); print('Updated sessions')"

# Test metrics retrieval
python -c "import golf_db; sessions = golf_db.get_unique_sessions(); if sessions: print(golf_db.get_session_metrics(sessions[0]['session_id']))"
```
Should compute and retrieve metrics without errors.
  </verify>
  <done>
- `update_session_metrics()` computes and stores aggregate stats per session
- `get_session_metrics()` retrieves stored metrics efficiently
- `update_all_session_metrics()` backfills all existing sessions
- Metrics auto-update after shot add/delete operations
- `session_stats` table is populated and queryable
  </done>
</task>

<task type="auto">
  <name>Add tests for versioning and metrics</name>
  <files>tests/unit/test_ml_models.py, tests/test_golf_db.py</files>
  <action>
Add test coverage for new functionality.

**In tests/unit/test_ml_models.py:**

Add test class `TestModelVersioning` (if ML deps available):
1. `test_save_model_creates_metadata()` - Train model, save it, verify .metadata.json exists
2. `test_load_model_with_metadata()` - Load model with metadata, verify ModelMetadata returned
3. `test_load_model_without_metadata()` - Load model without metadata (backward compat), verify no crash
4. `test_get_model_info()` - Call get_model_info(), verify metadata returned
5. `test_feature_name_mismatch_logs_warning()` - Mock metadata with wrong features, verify warning logged

**In tests/test_golf_db.py:**

Add test class `TestSessionMetrics`:
1. `test_update_session_metrics_computes_stats()` - Add 5 shots to session, call update_session_metrics(), verify stats computed
2. `test_get_session_metrics_returns_dict()` - Populate metrics, call get_session_metrics(), verify dict structure
3. `test_metrics_auto_update_after_add_shot()` - Add shot, verify session_stats updated
4. `test_update_all_session_metrics_backfills()` - Create 3 sessions, call update_all_session_metrics(), verify all 3 have metrics

Use test fixtures from conftest.py (`populated_golf_db`, `sample_shots_batch`).
  </action>
  <verify>
```bash
python -m unittest tests.unit.test_ml_models.TestModelVersioning -v
python -m unittest tests.test_golf_db.TestSessionMetrics -v
```
All tests should pass.
  </verify>
  <done>
- Test coverage for model versioning added
- Test coverage for session metrics population added
- Tests validate backward compatibility
- Tests verify metrics auto-update on shot operations
  </done>
</task>

</tasks>

<verification>
1. Run all tests: `python -m unittest discover -s tests`
2. Verify model metadata: `ls -la models/*.metadata.json`
3. Check session_stats populated: `sqlite3 golf_stats.db "SELECT COUNT(*) FROM session_stats"`
4. Test get_model_info: `python -c "from ml.train_models import get_model_info; from pathlib import Path; print(get_model_info(Path('models/distance_model.joblib')))"`
</verification>

<success_criteria>
- Model save/load includes metadata handling
- `get_model_info()` function provides lightweight metadata access
- Backward compatibility maintained for old models
- `update_session_metrics()` computes and stores aggregate stats
- `get_session_metrics()` retrieves metrics efficiently
- Session metrics auto-update after shot operations
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-stability/01-03-SUMMARY.md`
</output>
