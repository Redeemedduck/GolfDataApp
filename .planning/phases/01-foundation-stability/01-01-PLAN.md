---
phase: 01-foundation-stability
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - ml/__init__.py
  - ml/train_models.py
  - ml/classifiers.py
  - ml/anomaly_detection.py
  - local_coach.py
autonomous: true

must_haves:
  truths:
    - "App startup shows clear ML availability status"
    - "ML import failures surface immediately, not at feature use time"
    - "Missing dependencies show user-friendly error with install instructions"
  artifacts:
    - path: "ml/__init__.py"
      provides: "Explicit optional imports with feature flags"
      exports: ["ML_AVAILABLE", "ML_MISSING_DEPS"]
      min_lines: 50
    - path: "ml/train_models.py"
      provides: "HAS_ML_DEPS flag set via try/except"
      contains: "try:.*import.*except.*ImportError"
    - path: "local_coach.py"
      provides: "LocalCoach checks ML_AVAILABLE before loading models"
      contains: "if.*ML_AVAILABLE"
  key_links:
    - from: "local_coach.py"
      to: "ml/__init__.py"
      via: "imports ML_AVAILABLE flag"
      pattern: "from ml import.*ML_AVAILABLE"
    - from: "ml/__init__.py"
      to: "ml/train_models.py"
      via: "explicit import in try/except block"
      pattern: "try:.*from .train_models import"
---

<objective>
Replace fragile `__getattr__` lazy loading with explicit optional imports using try/except blocks and feature flags.

Purpose: Prevent late runtime errors when ML dependencies are missing. Surface import failures at app startup with clear user guidance, not during feature use.

Output: ML module with explicit imports, `ML_AVAILABLE` flag, and graceful degradation across all consuming code.
</objective>

<execution_context>
@/Users/max1/.claude/get-shit-done/workflows/execute-plan.md
@/Users/max1/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/research/PITFALLS.md

# Current implementation
@ml/__init__.py
@ml/train_models.py
@ml/classifiers.py
@ml/anomaly_detection.py
@local_coach.py
</context>

<tasks>

<task type="auto">
  <name>Refactor ml/__init__.py to explicit imports with feature flags</name>
  <files>ml/__init__.py</files>
  <action>
Replace the `__getattr__` lazy loading pattern with explicit try/except imports at module level.

Changes:
1. Remove `__getattr__` function entirely
2. Add explicit imports in try/except blocks for each submodule:
   - `train_models` → DistancePredictor, train_distance_model, load_model, save_model
   - `classifiers` → ShotShapeClassifier, ShotShape, classify_shot_shape
   - `anomaly_detection` → SwingFlawDetector, SwingFlaw, detect_swing_flaws
3. Export `ML_AVAILABLE` boolean flag (True if all imports succeeded)
4. Export `ML_MISSING_DEPS` list of missing dependencies (e.g., ["xgboost", "scikit-learn"])
5. If imports fail, set exports to None and populate ML_MISSING_DEPS
6. Add module-level docstring explaining feature flags

Example structure:
```python
ML_AVAILABLE = False
ML_MISSING_DEPS = []

try:
    from .train_models import DistancePredictor, train_distance_model, load_model, save_model
    ML_AVAILABLE = True
except ImportError as e:
    DistancePredictor = None
    ML_MISSING_DEPS.append("xgboost or scikit-learn")
```

Keep existing `__all__` exports for backward compatibility.
  </action>
  <verify>
```bash
python -c "import ml; print(f'ML_AVAILABLE: {ml.ML_AVAILABLE}'); print(f'Missing: {ml.ML_MISSING_DEPS}')"
```
Should print ML status without raising AttributeError.
  </verify>
  <done>
- `ml/__init__.py` no longer uses `__getattr__`
- `ML_AVAILABLE` flag is exported and accessible
- `ML_MISSING_DEPS` list is exported (empty if all deps present)
- Importing `ml` never raises ImportError (graceful degradation)
  </done>
</task>

<task type="auto">
  <name>Update consuming code to check ML_AVAILABLE before use</name>
  <files>local_coach.py</files>
  <action>
Update LocalCoach class to check ML availability flags before attempting to load models.

Changes in `local_coach.py`:
1. Import `ML_AVAILABLE` and `ML_MISSING_DEPS` from ml module
2. In `__init__`, check `ML_AVAILABLE` before calling `_load_ml_models()`
3. In `ml_available` property, return `ML_AVAILABLE` flag directly
4. Add `get_ml_status()` method returning dict with:
   - `available`: bool
   - `missing_deps`: list
   - `message`: user-friendly string (e.g., "ML features unavailable. Install: pip install scikit-learn xgboost")

Update the try/except block in `_load_ml_models()` to be defensive (check if classes are None before instantiating).

Do NOT change ml/train_models.py, ml/classifiers.py, or ml/anomaly_detection.py — those already have HAS_ML_DEPS checks.
  </action>
  <verify>
```bash
python -c "from local_coach import LocalCoach; coach = LocalCoach(); status = coach.get_ml_status(); print(status)"
```
Should print ML status dict without errors.
  </verify>
  <done>
- `local_coach.py` imports and checks `ML_AVAILABLE`
- LocalCoach gracefully degrades when ML unavailable
- `get_ml_status()` method provides user-facing diagnostics
  </done>
</task>

<task type="auto">
  <name>Add unit tests for ML import failure scenarios</name>
  <files>tests/unit/test_ml_models.py</files>
  <action>
Add test cases for graceful degradation when ML dependencies are missing.

Add new test class `TestMLImportFallback`:
1. `test_ml_available_flag_exists()` - Verify `ML_AVAILABLE` is accessible
2. `test_ml_missing_deps_is_list()` - Verify `ML_MISSING_DEPS` is a list
3. `test_local_coach_instantiates_without_ml()` - Mock `ML_AVAILABLE=False`, instantiate LocalCoach, verify no crash
4. `test_get_ml_status_returns_dict()` - Call LocalCoach().get_ml_status(), verify dict structure

Use `unittest.mock.patch` to simulate missing dependencies by patching `ml.ML_AVAILABLE` to False.

Place new tests at the end of the file, after existing test classes.
  </action>
  <verify>
```bash
python -m unittest tests.unit.test_ml_models.TestMLImportFallback -v
```
All tests should pass.
  </verify>
  <done>
- Test coverage for ML import failure scenarios added
- Tests verify graceful degradation behavior
- Tests pass in both ML-available and ML-unavailable scenarios
  </done>
</task>

</tasks>

<verification>
1. Run all unit tests: `python -m unittest discover -s tests/unit`
2. Import ml module in clean Python shell: `python -c "import ml; print(ml.ML_AVAILABLE)"`
3. Check LocalCoach instantiates: `python -c "from local_coach import LocalCoach; LocalCoach()"`
4. Verify no `__getattr__` in ml/__init__.py: `grep -n __getattr__ ml/__init__.py` (should be empty)
</verification>

<success_criteria>
- ML module imports succeed regardless of dependency availability
- `ML_AVAILABLE` flag accurately reflects dependency state
- `ML_MISSING_DEPS` lists missing packages when imports fail
- LocalCoach checks ML_AVAILABLE before attempting model operations
- All existing tests continue to pass
- New tests validate graceful degradation behavior
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-stability/01-01-SUMMARY.md`
</output>
