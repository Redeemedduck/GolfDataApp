---
phase: 04-monitoring-model-health
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - golf_db.py
  - ml/monitoring/drift_detector.py
  - tests/unit/test_monitoring.py
autonomous: true

must_haves:
  truths:
    - "Predictions are logged automatically when shots are saved (if model is loaded)"
    - "Drift detection runs automatically after session metrics are updated"
    - "Retraining is recommended when 3+ consecutive drift sessions detected"
    - "Prediction logging does not slow down or block shot imports on failure"
    - "Drift check does not crash update_session_metrics on failure"
  artifacts:
    - path: "golf_db.py"
      provides: "Prediction logging hook in save_shot(), drift check hook in update_session_metrics()"
      contains: "log_prediction"
    - path: "ml/monitoring/drift_detector.py"
      provides: "check_and_trigger_retraining() function for automated retraining flow"
      exports: ["DriftDetector", "check_and_trigger_retraining"]
    - path: "tests/unit/test_monitoring.py"
      provides: "Unit tests for DriftDetector and PerformanceTracker"
      min_lines: 80
  key_links:
    - from: "golf_db.py save_shot()"
      to: "ml/monitoring/performance_tracker.py"
      via: "PerformanceTracker.log_prediction() call after SQLite write"
      pattern: "log_prediction"
    - from: "golf_db.py update_session_metrics()"
      to: "ml/monitoring/drift_detector.py"
      via: "DriftDetector.check_session_drift() call after metrics computed"
      pattern: "check_session_drift"
---

<objective>
Wire prediction logging and drift detection into the existing data pipeline, add automated retraining trigger, and write unit tests.

Purpose: Without integration into save_shot() and update_session_metrics(), the monitoring module from Plan 01 would sit unused. This plan connects the monitoring system to the live data flow and validates it with tests.

Output: Prediction logging in save_shot(), drift checking in update_session_metrics(), check_and_trigger_retraining() function, comprehensive unit tests.
</objective>

<execution_context>
@/Users/max1/.claude/get-shit-done/workflows/execute-plan.md
@/Users/max1/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-monitoring-model-health/04-RESEARCH.md
@.planning/phases/04-monitoring-model-health/04-01-SUMMARY.md
@golf_db.py
@ml/monitoring/drift_detector.py
@ml/monitoring/performance_tracker.py
@ml/train_models.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wire prediction logging into save_shot() and drift detection into update_session_metrics()</name>
  <files>golf_db.py, ml/monitoring/drift_detector.py</files>
  <action>
**Part A: Add prediction logging to save_shot() in golf_db.py**

At the top of golf_db.py, add a lazy import for the monitoring module (following the existing pattern of optional imports):

```python
# Monitoring imports (lazy, non-blocking)
_performance_tracker = None
_distance_predictor = None

def _get_performance_tracker():
    global _performance_tracker
    if _performance_tracker is None:
        try:
            from ml.monitoring import PerformanceTracker
            _performance_tracker = PerformanceTracker()
        except ImportError:
            pass
    return _performance_tracker

def _get_distance_predictor():
    global _distance_predictor
    if _distance_predictor is None:
        try:
            from ml.train_models import DistancePredictor, DISTANCE_MODEL_PATH
            from pathlib import Path
            if Path(DISTANCE_MODEL_PATH).exists():
                _distance_predictor = DistancePredictor()
                _distance_predictor.load()
        except (ImportError, Exception):
            pass
    return _distance_predictor
```

In save_shot(), after the SQLite write succeeds (after `conn.close()` on line ~509, before the Supabase sync), add:

```python
# Log prediction for drift monitoring (non-blocking)
try:
    tracker = _get_performance_tracker()
    predictor = _get_distance_predictor()
    carry = payload.get('carry', 0)
    ball_speed = payload.get('ball_speed', 0)
    if tracker and predictor and predictor.is_loaded() and carry and carry > 0 and carry != 99999 and ball_speed and ball_speed > 0:
        prediction = predictor.predict(
            ball_speed=ball_speed,
            launch_angle=payload.get('launch_angle', 12.0),
            back_spin=payload.get('back_spin', 2500),
            club_speed=payload.get('club_speed'),
            attack_angle=payload.get('attack_angle'),
            dynamic_loft=payload.get('dynamic_loft'),
        )
        model_version = predictor.metadata.version if predictor.metadata else "unknown"
        tracker.log_prediction(
            shot_id=payload['shot_id'],
            club=payload.get('club', ''),
            predicted_carry=prediction.predicted_value,
            actual_carry=carry,
            model_version=model_version,
        )
except Exception as e:
    logger.debug(f"Prediction logging skipped: {e}")
```

Key design: This is completely non-blocking. If any part fails (no model, no tracker, prediction error), it logs at DEBUG level and continues. The save_shot() function's primary job (SQLite + Supabase write) is never affected.

**Part B: Add drift detection to update_session_metrics() in golf_db.py**

At the end of update_session_metrics(), after `conn.commit()` and `conn.close()` (around line 2152), before the logger.info call, add:

```python
# Check for model drift after session metrics update (non-blocking)
try:
    from ml.monitoring import DriftDetector
    detector = DriftDetector()
    drift_result = detector.check_session_drift(session_id)
    if drift_result.get('has_drift'):
        logger.warning(
            "Model drift detected",
            extra={
                "session_id": session_id,
                "session_mae": drift_result.get('session_mae'),
                "baseline_mae": drift_result.get('baseline_mae'),
                "drift_pct": drift_result.get('drift_pct'),
                "consecutive": drift_result.get('consecutive_drift_sessions'),
                "recommendation": drift_result.get('recommendation'),
            }
        )
except Exception as e:
    logger.debug(f"Drift check skipped: {e}")
```

Again, fully non-blocking. Drift check failure never affects session metrics computation.

**Part C: Add check_and_trigger_retraining() to ml/monitoring/drift_detector.py**

Add this function at module level (not inside the class) in drift_detector.py:

```python
def check_and_trigger_retraining(session_id: str, auto_retrain: bool = False) -> Optional[dict]:
    """
    Check for drift and optionally trigger automated retraining.

    Args:
        session_id: Session that just completed
        auto_retrain: If True, automatically retrain on 3+ consecutive drift (default: False, alert only)

    Returns:
        Dict with drift status and retraining result (if triggered), or None if no drift
    """
    detector = DriftDetector()
    drift_status = detector.check_session_drift(session_id)

    if not drift_status.get('has_drift'):
        return None

    consecutive = drift_status.get('consecutive_drift_sessions', 0)
    should_retrain = consecutive >= 3

    if should_retrain and auto_retrain:
        logger.info(f"Auto-retraining triggered: {consecutive} consecutive drift sessions")
        try:
            import time
            from ml.train_models import DistancePredictor, HAS_MAPIE
            import golf_db

            start_time = time.time()
            predictor = DistancePredictor()
            df = golf_db.get_all_shots()

            if HAS_MAPIE and len(df) >= 1000:
                new_metadata = predictor.train_with_intervals(df=df, save=True)
            else:
                new_metadata = predictor.train(df=df, save=True)

            elapsed = time.time() - start_time
            drift_status['retraining_triggered'] = True
            drift_status['retraining_success'] = True
            drift_status['new_mae'] = new_metadata.metrics['mae']
            drift_status['retraining_time'] = elapsed

            logger.info(f"Auto-retraining completed: MAE {new_metadata.metrics['mae']:.2f} yards ({elapsed:.1f}s)")
        except Exception as e:
            drift_status['retraining_triggered'] = True
            drift_status['retraining_success'] = False
            drift_status['retraining_error'] = str(e)
            logger.error(f"Auto-retraining failed: {e}")
    elif should_retrain:
        drift_status['retraining_recommended'] = True
        drift_status['alert_message'] = (
            f"Model drift detected for {consecutive} sessions. "
            "Visit Model Health dashboard to retrain."
        )

    return drift_status
```

Add `from typing import Optional` import at top of drift_detector.py if not already present. Export `check_and_trigger_retraining` from `ml/monitoring/__init__.py`.
  </action>
  <verify>
Run `python -m py_compile golf_db.py && echo 'golf_db OK'`.
Run `python -m py_compile ml/monitoring/drift_detector.py && echo 'drift_detector OK'`.
Run `python -c "import golf_db; golf_db.init_db(); print('init OK')"`.
Run `python -m unittest discover -s tests` to confirm no regressions.
  </verify>
  <done>save_shot() logs predictions non-blockingly when model is loaded. update_session_metrics() checks drift non-blockingly after metrics computation. check_and_trigger_retraining() supports both alert-only and auto-retrain modes. All hooks use try/except so failures never affect primary data operations.</done>
</task>

<task type="auto">
  <name>Task 2: Write unit tests for DriftDetector and PerformanceTracker</name>
  <files>tests/unit/test_monitoring.py</files>
  <action>
Create `tests/unit/test_monitoring.py` with comprehensive tests. Use unittest and the existing conftest.py fixtures pattern. Tests should use temporary SQLite databases (not the production database).

Test setup:
- Create a temp directory with temporary SQLite database
- Call init_db()-equivalent table creation on the temp database
- Patch golf_db.SQLITE_DB_PATH to use temp database

**PerformanceTracker tests:**

1. `test_log_prediction_stores_record` - Log a prediction, verify row exists in model_predictions with correct shot_id, predicted_value, actual_value, absolute_error.

2. `test_log_prediction_computes_absolute_error` - Log prediction (predicted=150, actual=140), verify absolute_error=10.

3. `test_log_prediction_skips_sentinel_carry` - Call log_prediction with actual_carry=99999, verify no row inserted.

4. `test_log_prediction_skips_zero_carry` - Call log_prediction with actual_carry=0, verify no row inserted.

5. `test_log_prediction_handles_error_gracefully` - Pass invalid db_path, verify no exception raised (graceful degradation).

6. `test_get_session_predictions_returns_dataframe` - Log several predictions, query by session, verify correct DataFrame shape.

**DriftDetector tests:**

7. `test_check_session_drift_insufficient_predictions` - Session with <5 predictions returns has_drift=False with message.

8. `test_check_session_drift_building_baseline` - Session with valid predictions but <10 baseline sessions returns has_drift=False with "Building baseline" message.

9. `test_check_session_drift_no_drift` - Insert 15 model_performance records with MAE=8, then check a session with MAE=9 (12.5% above baseline, below 30% threshold). Verify has_drift=False.

10. `test_check_session_drift_detects_drift` - Insert 15 model_performance records with MAE=8, then check a session with MAE=12 (50% above baseline). Verify has_drift=True, drift_pct>0.30.

11. `test_consecutive_drift_counting` - Insert 3 model_performance records with has_drift=1. Verify get_consecutive_drift_count() returns 3.

12. `test_consecutive_drift_resets_on_clean_session` - Insert records: drift, drift, clean, drift. Verify get_consecutive_drift_count() returns 1 (only most recent).

13. `test_recommendation_urgent_at_three_consecutive` - After 3 consecutive drift sessions, verify recommendation contains "URGENT" or "Retrain".

14. `test_check_and_trigger_retraining_alert_only` - Call check_and_trigger_retraining with auto_retrain=False after drift. Verify retraining_recommended=True but retraining_triggered not set.

Each test should:
- Create its own temp database (use tempfile.mkdtemp)
- Initialize tables (run the CREATE TABLE statements)
- Clean up after itself
- Use DriftDetector(db_path=temp_db) and PerformanceTracker(db_path=temp_db) constructors

Use setUp/tearDown for temp database management. Group into two test classes: TestPerformanceTracker and TestDriftDetector.

For tests that need predictions in model_predictions, directly INSERT test data into the temp database rather than going through the full prediction pipeline (faster, more isolated).
  </action>
  <verify>
Run `python -m unittest tests.unit.test_monitoring -v` and verify all tests pass.
Run `python -m unittest discover -s tests` to verify no regressions.
  </verify>
  <done>14+ unit tests covering PerformanceTracker (prediction logging, sentinel filtering, error handling) and DriftDetector (drift detection, baseline computation, consecutive counting, recommendations). All tests use isolated temp databases and clean up after themselves.</done>
</task>

</tasks>

<verification>
1. `python -m py_compile golf_db.py` succeeds
2. `python -c "from ml.monitoring import check_and_trigger_retraining; print('OK')"` succeeds
3. `python -m unittest tests.unit.test_monitoring -v` — all 14+ tests pass
4. `python -m unittest discover -s tests` — no regressions in existing tests
5. Grep for `log_prediction` in golf_db.py confirms integration in save_shot()
6. Grep for `check_session_drift` in golf_db.py confirms integration in update_session_metrics()
</verification>

<success_criteria>
- save_shot() calls PerformanceTracker.log_prediction() after successful SQLite write (non-blocking)
- update_session_metrics() calls DriftDetector.check_session_drift() after metrics computed (non-blocking)
- check_and_trigger_retraining() supports alert-only (default) and auto-retrain modes
- 14+ unit tests pass covering prediction logging, drift detection, consecutive counting, recommendations
- Zero regressions in existing test suite
</success_criteria>

<output>
After completion, create `.planning/phases/04-monitoring-model-health/04-02-SUMMARY.md`
</output>
