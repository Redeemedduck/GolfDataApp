---
phase: 04-monitoring-model-health
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - golf_db.py
  - ml/monitoring/__init__.py
  - ml/monitoring/drift_detector.py
  - ml/monitoring/performance_tracker.py
autonomous: true

must_haves:
  truths:
    - "model_predictions table exists and stores predicted vs actual carry for each shot"
    - "model_performance table exists and stores session-level MAE, baseline MAE, and drift flag"
    - "DriftDetector computes session MAE and compares against adaptive baseline (median of last 20 sessions)"
    - "DriftDetector counts consecutive drift sessions and recommends retraining at 3+"
    - "PerformanceTracker logs individual predictions and computes session-level metrics"
  artifacts:
    - path: "golf_db.py"
      provides: "model_predictions and model_performance table creation in init_db()"
      contains: "model_predictions"
    - path: "ml/monitoring/__init__.py"
      provides: "Package exports for DriftDetector and PerformanceTracker"
      exports: ["DriftDetector", "PerformanceTracker"]
    - path: "ml/monitoring/drift_detector.py"
      provides: "Core drift detection logic with adaptive thresholds"
      exports: ["DriftDetector"]
    - path: "ml/monitoring/performance_tracker.py"
      provides: "Prediction logging and session metrics computation"
      exports: ["PerformanceTracker"]
  key_links:
    - from: "ml/monitoring/drift_detector.py"
      to: "golf_db.py"
      via: "SQLite queries on model_performance table"
      pattern: "model_performance"
    - from: "ml/monitoring/performance_tracker.py"
      to: "golf_db.py"
      via: "SQLite queries on model_predictions table"
      pattern: "model_predictions"
---

<objective>
Create the database schema and core monitoring module for model drift detection.

Purpose: Establish the data layer (2 new tables) and core algorithms (DriftDetector, PerformanceTracker) that all other Phase 4 plans depend on. Without prediction storage and drift computation, no monitoring or dashboard is possible.

Output: 2 new SQLite tables in init_db(), ml/monitoring/ package with DriftDetector and PerformanceTracker classes.
</objective>

<execution_context>
@/Users/max1/.claude/get-shit-done/workflows/execute-plan.md
@/Users/max1/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-monitoring-model-health/04-RESEARCH.md
@golf_db.py
@ml/train_models.py
@ml/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add model_predictions and model_performance tables to init_db()</name>
  <files>golf_db.py</files>
  <action>
Add two new CREATE TABLE statements to the `init_db()` function in golf_db.py, after the existing `session_stats` table creation (around line 199, before `conn.commit()`).

Table 1 - model_predictions:
```sql
CREATE TABLE IF NOT EXISTS model_predictions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    shot_id TEXT NOT NULL,
    club TEXT,
    predicted_value REAL,
    actual_value REAL,
    absolute_error REAL,
    model_version TEXT,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (shot_id) REFERENCES shots(shot_id)
);
CREATE INDEX IF NOT EXISTS idx_predictions_shot ON model_predictions(shot_id);
CREATE INDEX IF NOT EXISTS idx_predictions_timestamp ON model_predictions(timestamp);
```

Table 2 - model_performance:
```sql
CREATE TABLE IF NOT EXISTS model_performance (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    session_id TEXT NOT NULL,
    session_mae REAL,
    baseline_mae REAL,
    has_drift INTEGER DEFAULT 0,
    drift_pct REAL,
    consecutive_drift INTEGER DEFAULT 0,
    model_version TEXT,
    recommendation TEXT,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
CREATE INDEX IF NOT EXISTS idx_performance_session ON model_performance(session_id);
CREATE INDEX IF NOT EXISTS idx_performance_timestamp ON model_performance(timestamp);
```

Follow the existing pattern: CREATE TABLE IF NOT EXISTS, indexes created immediately after table. Use INTEGER for has_drift (SQLite boolean convention matching existing patterns). Add drift_pct, consecutive_drift, model_version, and recommendation columns for richer dashboard queries.

Do NOT modify any existing tables or functions. Only add new CREATE TABLE and CREATE INDEX statements.
  </action>
  <verify>
Run `python -c "import golf_db; golf_db.init_db(); print('OK')"` to confirm no errors.
Run `python -c "import sqlite3; conn = sqlite3.connect('golf_stats.db'); c = conn.cursor(); c.execute('SELECT name FROM sqlite_master WHERE type=\"table\"'); print([r[0] for r in c.fetchall()])"` to confirm both new tables exist.
  </verify>
  <done>init_db() creates model_predictions and model_performance tables with indexes. Existing tables unaffected. All existing tests pass.</done>
</task>

<task type="auto">
  <name>Task 2: Create ml/monitoring/ package with DriftDetector and PerformanceTracker</name>
  <files>ml/monitoring/__init__.py, ml/monitoring/drift_detector.py, ml/monitoring/performance_tracker.py</files>
  <action>
Create the `ml/monitoring/` directory and three files.

**ml/monitoring/__init__.py:**
Export DriftDetector and PerformanceTracker. Follow the ml/__init__.py pattern with graceful degradation.

**ml/monitoring/performance_tracker.py:**
Create `PerformanceTracker` class with these methods:

1. `__init__(self, db_path: str = None)` - Accept optional db_path, default to golf_db.SQLITE_DB_PATH.

2. `log_prediction(self, shot_id: str, club: str, predicted_carry: float, actual_carry: float, model_version: str) -> None` - Insert a row into model_predictions with shot_id, club, predicted_value, actual_value, absolute_error (abs(predicted - actual)), model_version, and timestamp. Wrap in try/except, log errors but don't raise (non-blocking). Skip if actual_carry is None, 0, or 99999 (sentinel values).

3. `log_session_predictions(self, session_id: str, df: pd.DataFrame, predictor) -> int` - Batch prediction logging for an entire session. Takes session DataFrame and a DistancePredictor instance. For each shot with valid carry (>0, not 99999) and valid ball_speed (>0): call predictor.predict(ball_speed, launch_angle, back_spin) to get predicted carry, then log_prediction(). Return count of predictions logged. Wrap the whole method in try/except with logging (graceful degradation - never crash the import flow).

4. `get_session_predictions(self, session_id: str) -> pd.DataFrame` - Query model_predictions joined with shots WHERE session_id matches. Return DataFrame.

5. `get_prediction_history(self, limit: int = 50) -> pd.DataFrame` - Query model_performance table ordered by timestamp DESC, LIMIT limit. Return DataFrame.

Import golf_db for SQLITE_DB_PATH. Import sqlite3, pandas, numpy. Use `from utils.logging_config import get_logger`.

**ml/monitoring/drift_detector.py:**
Create `DriftDetector` class with these methods:

1. `__init__(self, db_path: str = None, drift_threshold_pct: float = 0.30, min_predictions: int = 5, baseline_sessions: int = 20, min_baseline_sessions: int = 10)` - Configurable thresholds per research recommendations.

2. `check_session_drift(self, session_id: str) -> dict` - Core method. Steps:
   a. Get predictions for this session from model_predictions (JOIN with shots on shot_id WHERE session_id matches).
   b. If fewer than min_predictions, return `{'has_drift': False, 'message': f'Need at least {min_predictions} predictions (have {n})'}`.
   c. Compute session_mae = mean of absolute_error column.
   d. Get baseline_mae by querying median of session_mae from model_performance table (last baseline_sessions records).
   e. If fewer than min_baseline_sessions records, store the record and return `{'has_drift': False, 'session_mae': session_mae, 'message': 'Building baseline (need N+ sessions)'}`.
   f. Compute drift_pct = (session_mae - baseline_mae) / baseline_mae.
   g. Determine has_drift = drift_pct > drift_threshold_pct.
   h. Count consecutive drift sessions from most recent records in model_performance (walk backward, count while has_drift=1).
   i. Generate recommendation string: "URGENT: Retrain model" if consecutive >= 3, "Monitor closely" if has_drift, else "Model performing within expected range".
   j. Store a new row in model_performance (session_id, session_mae, baseline_mae, has_drift, drift_pct, consecutive_drift, model_version from metadata if available, recommendation).
   k. Return dict with: has_drift, session_mae, baseline_mae, drift_pct, consecutive_drift_sessions, recommendation.

3. `get_drift_history(self, limit: int = 50) -> pd.DataFrame` - Query model_performance ORDER BY timestamp DESC LIMIT limit.

4. `get_consecutive_drift_count(self) -> int` - Count consecutive drift sessions from most recent (public accessor).

All SQLite operations use the db_path passed to constructor (default golf_db.SQLITE_DB_PATH). All methods use try/except with logging - never crash the caller. Use `from utils.logging_config import get_logger`.

Key design decisions per research:
- 30% adaptive threshold (not fixed yard threshold) to reduce false alarms
- Median baseline (not mean) to be robust to outlier sessions
- 3 consecutive drift sessions before "urgent retrain" recommendation
- Minimum 5 predictions per session, minimum 10 sessions for baseline
  </action>
  <verify>
Run `python -c "from ml.monitoring import DriftDetector, PerformanceTracker; print('Imports OK')"`.
Run `python -m py_compile ml/monitoring/__init__.py && python -m py_compile ml/monitoring/drift_detector.py && python -m py_compile ml/monitoring/performance_tracker.py && echo 'Compile OK'`.
Run `python -m unittest discover -s tests` to confirm no regressions.
  </verify>
  <done>ml/monitoring/ package exists with DriftDetector (adaptive threshold drift detection, consecutive counting, recommendation generation) and PerformanceTracker (prediction logging, session batch logging, history queries). Both classes use configurable parameters, graceful error handling, and proper logging. All imports resolve without errors.</done>
</task>

</tasks>

<verification>
1. `python -c "import golf_db; golf_db.init_db()"` succeeds without errors
2. Both model_predictions and model_performance tables exist in SQLite
3. `from ml.monitoring import DriftDetector, PerformanceTracker` succeeds
4. All existing tests pass: `python -m unittest discover -s tests`
5. `python -m py_compile` succeeds for all new and modified files
</verification>

<success_criteria>
- model_predictions and model_performance tables created by init_db() with proper indexes
- DriftDetector.check_session_drift() returns structured dict with drift status, MAE values, and recommendation
- PerformanceTracker.log_prediction() stores prediction records without blocking callers on error
- PerformanceTracker.log_session_predictions() batch-processes session shots through predictor
- All code compiles, all existing tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/04-monitoring-model-health/04-01-SUMMARY.md`
</output>
