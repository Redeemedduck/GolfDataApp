---
phase: 03-ml-enhancement-coaching
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - ml/train_models.py
  - ml/tuning.py
  - requirements.txt
  - tests/unit/test_prediction_intervals.py
autonomous: true

must_haves:
  truths:
    - "Predictions return lower_bound, upper_bound, and confidence_level alongside point estimate"
    - "XGBoost training uses dataset-size-aware regularization (higher lambda/alpha for small datasets)"
    - "MAPIE wraps the XGBoost model and produces conformalized prediction intervals"
    - "Graceful fallback to point-estimate-only when MAPIE is not installed or data < 1000 shots"
  artifacts:
    - path: "ml/train_models.py"
      provides: "DistancePredictor extended with predict_with_intervals() and train_with_intervals()"
      contains: "predict_with_intervals"
    - path: "ml/tuning.py"
      provides: "get_small_dataset_params() returns XGBoost hyperparams tuned by dataset size"
      exports: ["get_small_dataset_params"]
    - path: "requirements.txt"
      provides: "MAPIE and scikit-learn version requirements"
      contains: "mapie"
    - path: "tests/unit/test_prediction_intervals.py"
      provides: "Tests for interval prediction, tuning params, and graceful degradation"
  key_links:
    - from: "ml/train_models.py"
      to: "ml/tuning.py"
      via: "import get_small_dataset_params"
      pattern: "from .tuning import get_small_dataset_params"
    - from: "ml/train_models.py"
      to: "mapie.regression"
      via: "MAPIE wrapping"
      pattern: "CrossConformalRegressor"
---

<objective>
Add MAPIE conformal prediction intervals to the XGBoost distance predictor and implement dataset-size-aware hyperparameter tuning.

Purpose: Users see "148-156 yards, 95% confidence" instead of bare point estimates, making predictions trustworthy and uncertainty transparent (COACH-03).

Output: Extended DistancePredictor with interval predictions, XGBoost tuning module, updated requirements, and tests.
</objective>

<execution_context>
@/Users/max1/.claude/get-shit-done/workflows/execute-plan.md
@/Users/max1/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@ml/train_models.py
@ml/__init__.py
@requirements.txt
@.planning/phases/03-ml-enhancement-coaching/03-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create XGBoost tuning module and extend DistancePredictor with MAPIE intervals</name>
  <files>ml/tuning.py, ml/train_models.py, requirements.txt</files>
  <action>
  1. **Create `ml/tuning.py`** with `get_small_dataset_params(n_samples: int) -> dict`:
     - n < 1000: max_depth=3, reg_lambda=3.0, reg_alpha=1.5, subsample=0.7, n_estimators=50, learning_rate=0.05, min_child_weight=3
     - n < 3000: max_depth=4, reg_lambda=2.0, reg_alpha=1.0, subsample=0.8, n_estimators=75, learning_rate=0.08, min_child_weight=2
     - n >= 3000: max_depth=5, reg_lambda=1.0, reg_alpha=0.5, subsample=0.8, n_estimators=100, learning_rate=0.1, min_child_weight=1
     - All include objective='reg:squarederror', random_state=42

  2. **Update `requirements.txt`**:
     - Add `mapie>=1.3.0`
     - Update `scikit-learn>=1.3.0` to `scikit-learn>=1.4.0` (MAPIE requires it)

  3. **Extend `ml/train_models.py`** — add to DistancePredictor class:
     - Import MAPIE with try/except: `from mapie.regression import CrossConformalRegressor` guarded by `HAS_MAPIE = True/False`
     - Import `from .tuning import get_small_dataset_params`
     - Add `self.mapie_model = None` in __init__
     - Add `train_with_intervals(self, df, confidence_level=0.95, save=True)`:
       - Call `prepare_features(df, target='carry')` to get X, y
       - Call `get_small_dataset_params(len(X))` for hyperparams
       - Split 70/30: train vs conformalization (train_test_split, test_size=0.3)
       - Train XGBRegressor with tuned params on X_train, y_train (with early_stopping_rounds=10 on 20% validation holdout from train set)
       - Create CrossConformalRegressor(base_model, method="plus", cv=5, confidence_level=confidence_level)
       - Call mapie_model.fit_conformalize(X_conform, y_conform)
       - Store self.model, self.mapie_model, self._feature_names
       - Compute metrics (MAE, RMSE, R2) on conformalization set
       - Create ModelMetadata with hyperparams from get_small_dataset_params
       - Save model AND mapie_model using joblib (save both as dict: {'base_model': model, 'mapie_model': mapie_model})
     - Add `predict_with_intervals(self, confidence_level=0.95, **kwargs) -> dict`:
       - Build feature array using existing _build_feature_array pattern (reuse predict() logic)
       - If self.mapie_model is None and not loaded, try to load
       - If MAPIE not available or mapie_model not trained, fall back to point prediction with a note: {'has_intervals': False}
       - Call mapie_model.predict_interval(X) -> y_pred, y_pis
       - Return dict: predicted_value, lower_bound, upper_bound, confidence_level, interval_width, has_intervals=True
     - Update `train()` to use `get_small_dataset_params()` instead of hardcoded params
     - Minimum sample check: if fewer than 1000 shots, set has_intervals=False and return point estimate only with message "Need 1000+ shots for confidence intervals"

  4. **Update `ml/__init__.py`**: Export `get_small_dataset_params` from tuning, add `HAS_MAPIE` to exports.

  Key implementation notes:
  - MAPIE import MUST be try/except (graceful degradation if not installed)
  - The save format changes: save as dict with both models. Add backward compat check in load() — if loaded model is XGBRegressor (not dict), treat as base model only
  - Keep existing predict() method unchanged for backward compat
  - The predict_with_intervals kwargs should match predict() signature: ball_speed, launch_angle, back_spin, club_speed, attack_angle, dynamic_loft
  </action>
  <verify>
  python -c "from ml.tuning import get_small_dataset_params; p = get_small_dataset_params(500); assert p['max_depth'] == 3; p2 = get_small_dataset_params(2000); assert p2['max_depth'] == 4; print('Tuning OK')"
  python -c "from ml.train_models import DistancePredictor; p = DistancePredictor(); print('Predictor imports OK')"
  python -m py_compile ml/tuning.py
  python -m py_compile ml/train_models.py
  </verify>
  <done>
  - get_small_dataset_params returns correct params for 3 size tiers
  - DistancePredictor has train_with_intervals() and predict_with_intervals() methods
  - MAPIE import is guarded with try/except; HAS_MAPIE flag works
  - requirements.txt includes mapie>=1.3.0 and scikit-learn>=1.4.0
  - All files pass py_compile
  </done>
</task>

<task type="auto">
  <name>Task 2: Add unit tests for prediction intervals and tuning</name>
  <files>tests/unit/test_prediction_intervals.py</files>
  <action>
  Create `tests/unit/test_prediction_intervals.py` with unittest.TestCase:

  **TestGetSmallDatasetParams:**
  - test_very_small_dataset: n=500, assert max_depth=3, reg_lambda=3.0
  - test_small_dataset: n=2000, assert max_depth=4, reg_lambda=2.0
  - test_medium_dataset: n=5000, assert max_depth=5, reg_lambda=1.0
  - test_boundary_1000: n=1000, assert max_depth=4 (should fall into small tier)
  - test_boundary_3000: n=3000, assert max_depth=5 (should fall into medium tier)
  - test_all_have_required_keys: all tiers return keys: n_estimators, max_depth, learning_rate, reg_lambda, reg_alpha, subsample, objective, random_state

  **TestDistancePredictorIntervals:**
  - test_predict_with_intervals_no_mapie: mock HAS_MAPIE=False, call predict_with_intervals, assert has_intervals=False and predicted_value is a float (graceful fallback)
  - test_predict_with_intervals_returns_dict_keys: when mapie_model is mocked, verify return dict has: predicted_value, lower_bound, upper_bound, confidence_level, interval_width, has_intervals
  - test_predict_with_intervals_bounds_ordering: lower_bound < predicted_value < upper_bound
  - test_train_uses_tuned_params: mock train to verify get_small_dataset_params is called
  - test_predict_with_intervals_contract: verify that the dict returned by predict_with_intervals() contains EXACTLY the keys that local_coach.py will consume: {'predicted_value', 'lower_bound', 'upper_bound', 'confidence_level', 'interval_width', 'has_intervals'}. This is an integration contract test — if keys change here, local_coach.py breaks. Mock mapie_model, call predict_with_intervals, assert set(result.keys()) == expected_keys

  Use conftest.py fixtures (ml_test_dataframe) where applicable.
  Mock MAPIE imports to avoid requiring MAPIE in CI (MAPIE may not be in CI environment).
  Use unittest.mock.patch for HAS_MAPIE and CrossConformalRegressor.
  </action>
  <verify>
  python -m unittest tests.unit.test_prediction_intervals -v
  python -m py_compile tests/unit/test_prediction_intervals.py
  </verify>
  <done>
  - All tests pass
  - Tests cover: tuning params for 3 tiers, boundary conditions, graceful degradation without MAPIE, interval dict structure, bounds ordering, integration contract keys
  - Contract test verifies predict_with_intervals() returns keys matching local_coach.py expectations
  - No MAPIE installation required for tests to pass (mocked)
  </done>
</task>

</tasks>

<verification>
python -m py_compile ml/tuning.py && python -m py_compile ml/train_models.py
python -m unittest tests.unit.test_prediction_intervals -v
python -m unittest discover -s tests -v  # Ensure no regressions
</verification>

<success_criteria>
1. DistancePredictor.predict_with_intervals() returns {predicted_value, lower_bound, upper_bound, confidence_level, interval_width, has_intervals}
2. get_small_dataset_params() returns 3 tiers of regularization params
3. Graceful fallback when MAPIE not installed (has_intervals=False)
4. All new and existing tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/03-ml-enhancement-coaching/03-01-SUMMARY.md`
</output>
